* Jupyter notebook + Spark + Scala + R
** Running container
  #+begin_src bash
  docker compose up -d
  #+end_src

** Accessing notebooks
   after running container for first time notebooks should be available in ~work~ volumen created in ~all_spark_notebook~

** Using Spark (example)
  #+begin_src Python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("SparkExample").getOrCreate()
data = [("Alice", 34), ("Bob", 45), ("Cathy", 29)]
columns = ["Name", "Age"]

df = spark.createDataFrame(data, columns)
df.show()
  #+end_src
